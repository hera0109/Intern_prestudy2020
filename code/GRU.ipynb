{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from pytorch_transformers import BertTokenizer\n",
    "from torchtext import data \n",
    "from torchtext.data import TabularDataset\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "lr = 0.001\n",
    "EPOCHS = 10\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True, batch_first=True, lower=True,  fix_length=500)\n",
    "LABEL = data.Field(sequential=False, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = TabularDataset.splits(\n",
    "        path='.', train='../data/imdb/train.csv', test='../data/imdb/test.csv', format='csv',\n",
    "        fields=[('text', TEXT), ('label', LABEL)], skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polish_data(data):\n",
    "    for example in data.examples:\n",
    "        temp = [x.replace(\"<br\",\"\") for x in vars(example)['text']]\n",
    "        temp = [''.join(c for c in x if c not in string.punctuation) for x in temp]\n",
    "        vars(example)['text'] = temp\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "polish_data(train_data)\n",
    "polish_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_data.split(random_state = random.seed(SEED), \n",
    "                                        split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, min_freq=5)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "vocab_size = len(TEXT.vocab)\n",
    "label_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('Using GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('Using CPU')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "        (train_data, val_data, test_data), batch_size=BATCH_SIZE,\n",
    "        device=device, sort_key=lambda x: len(vars(x)),\n",
    "        sort_within_batch=False, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, n_layers, hidden_dim, embed_dim,\n",
    "                 n_vocab, n_classes=2, dropout=0.2):\n",
    "        super(RNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embed = nn.Embedding(n_vocab, embed_dim, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(embed_dim, self.hidden_dim,\n",
    "                          num_layers=self.n_layers,\n",
    "                          batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h_0 = self.__init__state(batch_size=batch_size)\n",
    "        \n",
    "        x = self.embed(x)\n",
    "        x,_ = self.gru(x, h_0)\n",
    "        h_t = x[:,-1,:]\n",
    "        \n",
    "        self.dropout(h_t)\n",
    "        logit = self.out(h_t)\n",
    "        return logit\n",
    "    \n",
    "    def __init__state(self, batch_size=1):\n",
    "        weight = next(self.parameters()).data\n",
    "        return weight.new(self.n_layers, batch_size, \n",
    "                          self.hidden_dim).zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (embed): Embedding(27438, 400, padding_idx=0)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (gru): GRU(400, 256, batch_first=True)\n",
      "  (rnn): RNN(400, 256, batch_first=True)\n",
      "  (out): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_layers = 1\n",
    "hidden_dim = 256\n",
    "embed_dim = 400\n",
    "dropout = 0.5\n",
    "\n",
    "model = GRU(n_layers, hidden_dim, embed_dim, vocab_size, \n",
    "            2, dropout)\n",
    "print(model)\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, iterator):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        x, y = batch.text.to(device), batch.label.to(device)\n",
    "        predictions = model(x).squeeze(1)\n",
    "        y.data.sub_(1)\n",
    "        loss = F.cross_entropy(predictions, y)\n",
    "        acc = binary_accuracy(predictions, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss/len(iterator) , epoch_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            x, y = batch.text.to(device), batch.label.to(device)\n",
    "            predictions = model(x).squeeze(1)\n",
    "            y.data.sub_(1)\n",
    "            loss = F.cross_entropy(predictions, y, reduction='sum')\n",
    "            acc = binary_accuracy(predictions, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1] Train loss: 0.011 | Train acc: 1.00 | Val loss : 42.75 | Val accuracy :  0.88\n",
      "[Epoch: 2] Train loss: 0.008 | Train acc: 1.00 | Val loss : 40.80 | Val accuracy :  0.88\n",
      "[Epoch: 3] Train loss: 0.005 | Train acc: 1.00 | Val loss : 46.09 | Val accuracy :  0.87\n",
      "[Epoch: 4] Train loss: 0.006 | Train acc: 1.00 | Val loss : 39.27 | Val accuracy :  0.87\n",
      "[Epoch: 5] Train loss: 0.012 | Train acc: 1.00 | Val loss : 45.68 | Val accuracy :  0.88\n",
      "[Epoch: 6] Train loss: 0.010 | Train acc: 1.00 | Val loss : 39.56 | Val accuracy :  0.88\n",
      "[Epoch: 7] Train loss: 0.008 | Train acc: 1.00 | Val loss : 40.97 | Val accuracy :  0.88\n",
      "[Epoch: 8] Train loss: 0.004 | Train acc: 1.00 | Val loss : 46.93 | Val accuracy :  0.88\n",
      "[Epoch: 9] Train loss: 0.009 | Train acc: 1.00 | Val loss : 37.39 | Val accuracy :  0.88\n",
      "[Epoch: 10] Train loss: 0.006 | Train acc: 1.00 | Val loss : 42.46 | Val accuracy :  0.88\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, optimizer, train_iter)\n",
    "    val_loss, val_accuracy = evaluate(model, val_iter, criterion)\n",
    "\n",
    "    print(\"[Epoch: %d] Train loss: %.3f | Train acc: %.2f | Val loss : %5.2f | Val accuracy : %5.2f\" \n",
    "          % (epoch+1,train_loss,train_acc, val_loss, val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 오차: 49.290 | 테스트 정확도: 0.864\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iter, criterion)\n",
    "print('테스트 오차: %.3f | 테스트 정확도: %.3f' % (test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
